{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-29T17:14:32.896673Z",
     "iopub.status.busy": "2025-03-29T17:14:32.896361Z",
     "iopub.status.idle": "2025-03-29T17:14:35.378102Z",
     "shell.execute_reply": "2025-03-29T17:14:35.377187Z",
     "shell.execute_reply.started": "2025-03-29T17:14:32.896648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import tqdm\n",
    "import json\n",
    "import datasets\n",
    "from typing import List\n",
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:14:35.379543Z",
     "iopub.status.busy": "2025-03-29T17:14:35.379074Z",
     "iopub.status.idle": "2025-03-29T17:14:35.512816Z",
     "shell.execute_reply": "2025-03-29T17:14:35.511618Z",
     "shell.execute_reply.started": "2025-03-29T17:14:35.379519Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:14:35.539185Z",
     "iopub.status.busy": "2025-03-29T17:14:35.538969Z",
     "iopub.status.idle": "2025-03-29T17:14:35.801760Z",
     "shell.execute_reply": "2025-03-29T17:14:35.800797Z",
     "shell.execute_reply.started": "2025-03-29T17:14:35.539166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:14:35.803004Z",
     "iopub.status.busy": "2025-03-29T17:14:35.802690Z",
     "iopub.status.idle": "2025-03-29T17:14:35.809973Z",
     "shell.execute_reply": "2025-03-29T17:14:35.809143Z",
     "shell.execute_reply.started": "2025-03-29T17:14:35.802971Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:14:35.880929Z",
     "iopub.status.busy": "2025-03-29T17:14:35.880729Z",
     "iopub.status.idle": "2025-03-29T17:14:35.898984Z",
     "shell.execute_reply": "2025-03-29T17:14:35.898197Z",
     "shell.execute_reply.started": "2025-03-29T17:14:35.880911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, tokenizer_model=\"gpt2\"):\n",
    "        self.enc = tiktoken.get_encoding(tokenizer_model)\n",
    "        self.tokenizer_model = tokenizer_model\n",
    "\n",
    "        self.n_words = self.enc.n_vocab\n",
    "        self.bos_id = None\n",
    "        self.eos_id = self.enc.eot_token\n",
    "        self.pad_id = None\n",
    "\n",
    "    def encode(self, s: str, bos: bool = False, eos: bool = False) -> List[int]:\n",
    "        t = self.enc.encode(s)\n",
    "        if bos and self.bos_id is not None:\n",
    "            t = [self.bos_id] + t\n",
    "        if eos and self.eos_id is not None:\n",
    "            t = t + [self.eos_id]\n",
    "        return t\n",
    "\n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        return self.enc.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:14:35.900034Z",
     "iopub.status.busy": "2025-03-29T17:14:35.899805Z",
     "iopub.status.idle": "2025-03-29T17:14:35.913718Z",
     "shell.execute_reply": "2025-03-29T17:14:35.913113Z",
     "shell.execute_reply.started": "2025-03-29T17:14:35.900015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(tokenizer_model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:14:35.914671Z",
     "iopub.status.busy": "2025-03-29T17:14:35.914444Z",
     "iopub.status.idle": "2025-03-29T17:14:35.932927Z",
     "shell.execute_reply": "2025-03-29T17:14:35.932148Z",
     "shell.execute_reply.started": "2025-03-29T17:14:35.914653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:14:36.052352Z",
     "iopub.status.busy": "2025-03-29T17:14:36.052047Z",
     "iopub.status.idle": "2025-03-29T17:14:36.101821Z",
     "shell.execute_reply": "2025-03-29T17:14:36.100932Z",
     "shell.execute_reply.started": "2025-03-29T17:14:36.052328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 50304\n",
    "batch_size = 32\n",
    "block_size = 512\n",
    "max_iters = 1\n",
    "eval_interval = 1000\n",
    "learning_rate = 1e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 256\n",
    "n_embd = 512\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.3\n",
    "\n",
    "target_batch_size = 8192 * 2\n",
    "gradient_accumulation_steps = target_batch_size // batch_size\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:14:36.267655Z",
     "iopub.status.busy": "2025-03-29T17:14:36.267356Z",
     "iopub.status.idle": "2025-03-29T17:14:36.271294Z",
     "shell.execute_reply": "2025-03-29T17:14:36.270523Z",
     "shell.execute_reply.started": "2025-03-29T17:14:36.267634Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:14:36.478195Z",
     "iopub.status.busy": "2025-03-29T17:14:36.477913Z",
     "iopub.status.idle": "2025-03-29T17:14:36.481877Z",
     "shell.execute_reply": "2025-03-29T17:14:36.481050Z",
     "shell.execute_reply.started": "2025-03-29T17:14:36.478175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(f\"Using {torch.cuda.device_count()} GPUs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:14:36.683468Z",
     "iopub.status.busy": "2025-03-29T17:14:36.683124Z",
     "iopub.status.idle": "2025-03-29T17:14:36.687455Z",
     "shell.execute_reply": "2025-03-29T17:14:36.686492Z",
     "shell.execute_reply.started": "2025-03-29T17:14:36.683439Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def encode(s): return tokenizer.encode(s, bos=False, eos=False)\n",
    "\n",
    "def decode(l):\n",
    "\ttry:\n",
    "\t\treturn tokenizer.decode(l)\n",
    "\texcept:\n",
    "\t\treturn \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:02.990214Z",
     "iopub.status.busy": "2025-03-29T17:15:02.989890Z",
     "iopub.status.idle": "2025-03-29T17:15:04.676958Z",
     "shell.execute_reply": "2025-03-29T17:15:04.675989Z",
     "shell.execute_reply.started": "2025-03-29T17:15:02.990194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:04.678429Z",
     "iopub.status.busy": "2025-03-29T17:15:04.678016Z",
     "iopub.status.idle": "2025-03-29T17:15:04.683771Z",
     "shell.execute_reply": "2025-03-29T17:15:04.682983Z",
     "shell.execute_reply.started": "2025-03-29T17:15:04.678404Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds = ds.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:04.685378Z",
     "iopub.status.busy": "2025-03-29T17:15:04.685079Z",
     "iopub.status.idle": "2025-03-29T17:15:04.701480Z",
     "shell.execute_reply": "2025-03-29T17:15:04.700813Z",
     "shell.execute_reply.started": "2025-03-29T17:15:04.685358Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.\\n\\nBeep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:04.702498Z",
     "iopub.status.busy": "2025-03-29T17:15:04.702219Z",
     "iopub.status.idle": "2025-03-29T17:15:04.716701Z",
     "shell.execute_reply": "2025-03-29T17:15:04.716079Z",
     "shell.execute_reply.started": "2025-03-29T17:15:04.702468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    texts = [encode(item['text'])[:block_size] for item in batch]  # Truncate to block_size\n",
    "    padded_texts = [t + [0] * (block_size - len(t)) for t in texts]  # Pad to 512\n",
    "    return {\n",
    "        'text': torch.tensor(padded_texts, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:04.784456Z",
     "iopub.status.busy": "2025-03-29T17:15:04.784140Z",
     "iopub.status.idle": "2025-03-29T17:15:04.788807Z",
     "shell.execute_reply": "2025-03-29T17:15:04.788115Z",
     "shell.execute_reply.started": "2025-03-29T17:15:04.784429Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "subset_indices = list(range(eval_iters))\n",
    "# train_indices = list(range(8000000))\n",
    "# dataset_train = Subset(ds['train'], train_indices)\n",
    "dataset_valid = Subset(ds['validation'], subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:05.015383Z",
     "iopub.status.busy": "2025-03-29T17:15:05.015093Z",
     "iopub.status.idle": "2025-03-29T17:15:05.019676Z",
     "shell.execute_reply": "2025-03-29T17:15:05.018718Z",
     "shell.execute_reply.started": "2025-03-29T17:15:05.015358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(ds['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(dataset_valid, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:07.262995Z",
     "iopub.status.busy": "2025-03-29T17:15:07.262671Z",
     "iopub.status.idle": "2025-03-29T17:15:07.275054Z",
     "shell.execute_reply": "2025-03-29T17:15:07.274288Z",
     "shell.execute_reply.started": "2025-03-29T17:15:07.262971Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"\n",
    "    Generates a causal (upper-triangular) mask for a sequence of length 'sz'.\n",
    "    Positions with True (or -inf when using additive masks) will be masked.\n",
    "    Here, we create an additive mask with -inf for masked positions.\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block using PyTorch's MultiheadAttention with an explicit causal mask.\"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        # PyTorch's MultiheadAttention\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=n_embd,\n",
    "            num_heads=n_head,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Expect input as (batch, seq, feature)\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x has shape (B, T, C)\n",
    "        T = x.size(1)\n",
    "        \n",
    "        # Pre-LayerNorm for attention\n",
    "        x_ln = self.ln1(x)\n",
    "        # Create a causal mask explicitly for the current sequence length\n",
    "        causal_mask = generate_square_subsequent_mask(T).to(x.device)\n",
    "        \n",
    "        # Self-attention: note that we pass attn_mask instead of is_causal\n",
    "        attn_output, _ = self.attn(\n",
    "            query=x_ln,\n",
    "            key=x_ln,\n",
    "            value=x_ln,\n",
    "            attn_mask=causal_mask,  # Using the explicit causal mask here\n",
    "            need_weights=False\n",
    "        )\n",
    "        x = x + attn_output\n",
    "        \n",
    "        # Feed-forward block with pre-LayerNorm\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Token and position embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        \n",
    "        # Final layer normalization and output projection\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying: share the weight matrix between token embeddings and the output projection\n",
    "        self.token_embedding_table.weight = self.lm_head.weight\n",
    "        \n",
    "        # Initialize weights for Linear and Embedding layers\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Obtain token embeddings and add positional embeddings\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)  # (B, T, C)\n",
    "            \n",
    "        # Final layer normalization and output projection to logits\n",
    "        x = self.ln_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        # Compute loss if targets are provided\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Given a sequence of indices 'idx', generate 'max_new_tokens' new tokens.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop the sequence to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # Get predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "            # Convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, vocab_size)\n",
    "            # Sample from the probability distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # Append the new token to the sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:07.487791Z",
     "iopub.status.busy": "2025-03-29T17:15:07.487506Z",
     "iopub.status.idle": "2025-03-29T17:15:07.491406Z",
     "shell.execute_reply": "2025-03-29T17:15:07.490630Z",
     "shell.execute_reply.started": "2025-03-29T17:15:07.487770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:13.059933Z",
     "iopub.status.busy": "2025-03-29T17:15:13.059623Z",
     "iopub.status.idle": "2025-03-29T17:15:16.057653Z",
     "shell.execute_reply": "2025-03-29T17:15:16.056842Z",
     "shell.execute_reply.started": "2025-03-29T17:15:13.059909Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.237888 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "model = torch.compile(model)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:20.365414Z",
     "iopub.status.busy": "2025-03-29T17:15:20.364855Z",
     "iopub.status.idle": "2025-03-29T17:15:20.370353Z",
     "shell.execute_reply": "2025-03-29T17:15:20.369329Z",
     "shell.execute_reply.started": "2025-03-29T17:15:20.365382Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_fused=True\n"
     ]
    }
   ],
   "source": [
    "fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "use_fused = fused_available and 'cuda' == str(device)\n",
    "print(f\"{use_fused=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:21.256712Z",
     "iopub.status.busy": "2025-03-29T17:15:21.256405Z",
     "iopub.status.idle": "2025-03-29T17:15:21.261449Z",
     "shell.execute_reply": "2025-03-29T17:15:21.260548Z",
     "shell.execute_reply.started": "2025-03-29T17:15:21.256692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2), eps=1e-8, fused=use_fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:22.394106Z",
     "iopub.status.busy": "2025-03-29T17:15:22.393726Z",
     "iopub.status.idle": "2025-03-29T17:15:22.398572Z",
     "shell.execute_reply": "2025-03-29T17:15:22.397727Z",
     "shell.execute_reply.started": "2025-03-29T17:15:22.394075Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "T_max = len(train_dataloader)\n",
    "warmup_steps = 0.01 * T_max\n",
    "scheduler = lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=4e-4, total_steps=T_max, pct_start=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:24.679316Z",
     "iopub.status.busy": "2025-03-29T17:15:24.679006Z",
     "iopub.status.idle": "2025-03-29T17:15:24.684397Z",
     "shell.execute_reply": "2025-03-29T17:15:24.683579Z",
     "shell.execute_reply.started": "2025-03-29T17:15:24.679292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# eval_interval = len(train_dataloader) // 5\n",
    "# eval_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:24.971136Z",
     "iopub.status.busy": "2025-03-29T17:15:24.970831Z",
     "iopub.status.idle": "2025-03-29T17:15:24.975108Z",
     "shell.execute_reply": "2025-03-29T17:15:24.974180Z",
     "shell.execute_reply.started": "2025-03-29T17:15:24.971113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"ckpt/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:25.406187Z",
     "iopub.status.busy": "2025-03-29T17:15:25.405905Z",
     "iopub.status.idle": "2025-03-29T17:15:25.411089Z",
     "shell.execute_reply": "2025-03-29T17:15:25.410057Z",
     "shell.execute_reply.started": "2025-03-29T17:15:25.406165Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with<|endoftext|>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = tokenizer.decode(tokenizer.encode(ds[\"train\"][0][\"text\"][:100], bos=True, eos=True))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:26.176406Z",
     "iopub.status.busy": "2025-03-29T17:15:26.176055Z",
     "iopub.status.idle": "2025-03-29T17:15:26.181296Z",
     "shell.execute_reply": "2025-03-29T17:15:26.180471Z",
     "shell.execute_reply.started": "2025-03-29T17:15:26.176380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # crop idx to the last block_size tokens\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        # get the predictions\n",
    "        logits, loss = model(idx_cond)\n",
    "        # focus only on the last time step\n",
    "        logits = logits[:, -1, :]  # becomes (B, C)\n",
    "        # apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "        # append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 32, 8192)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_accumulation_steps, batch_size, target_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"losses.txt\", \"w\") as f:\n",
    "\tf.write(\"Training Loss,Validation Loss,Output\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T17:15:28.219769Z",
     "iopub.status.busy": "2025-03-29T17:15:28.219468Z",
     "iopub.status.idle": "2025-03-29T17:32:06.791451Z",
     "shell.execute_reply": "2025-03-29T17:32:06.789785Z",
     "shell.execute_reply.started": "2025-03-29T17:15:28.219749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for iter, batch in enumerate(tqdm.notebook.tqdm(train_dataloader, total=len(train_dataloader))):\n",
    "    inputs, targets = batch['text'], batch['text']\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    with torch.autocast(device_type=str(device), dtype=torch.bfloat16):\n",
    "        logits, loss = model(inputs, targets)\n",
    "\n",
    "    loss = loss / gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    if (iter + 1) % gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "    if iter % (gradient_accumulation_steps * 2) == 0 or iter == max_iters - 1:\n",
    "        print(f\"Step {iter}: Performing validation\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            train_loss = loss.item() * gradient_accumulation_steps\n",
    "            for batch in tqdm.notebook.tqdm(valid_dataloader, total=len(valid_dataloader)):\n",
    "                inputs, targets = batch['text'], batch['text']\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                _, loss = model(inputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            torch.save(model.state_dict(), f\"ckpt/ckpt_{iter}.pt\")\n",
    "            print(f\"Train loss: {train_loss:.4f}\")\n",
    "            print(f\"Validation loss: {val_loss / len(valid_dataloader):.4f}\")\n",
    "\n",
    "            prompt = \"Hello I\"\n",
    "            prompt = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "            output = decode(generate(model, prompt, max_new_tokens=50)[0].tolist())\n",
    "            print(output)\n",
    "            with open(\"losses.txt\", \"a\") as f:\n",
    "                f.write(f\"{train_loss},{val_loss / len(valid_dataloader)},\\\"{output}\\\"\\n\")\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"final_model_tiny_stories_tiktoken.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"final_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # model = GPTLanguageModel()\n",
    "# # model = model.to(device)\n",
    "# model.load_state_dict(torch.load(\"/kaggle/working/ckpt/ckpt_5625.pt\", weights_only=True))\n",
    "\n",
    "# # model.eval()\n",
    "# # model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a girl who who who who who who who who who who who who who cafe unders bag who r Discipline nipplebaGB Compl014Thom Martial Roc Come.................. Shinyvenant gearing slang Help\n"
     ]
    }
   ],
   "source": [
    "prompt = \"There was a girl who\"\n",
    "\n",
    "prompt = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "print(decode(generate(model, prompt, max_new_tokens=50)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, a little girl named Lily foundastrouseden stealth (' linebacker significancerors implement Drink 188atars ................ backyardReady perenn careers cookies suchchairs Elections throb loot campaigns break sky sky sky sky sky sky skyWhen all all all all all all all all all all all all all all all all all all\n"
     ]
    }
   ],
   "source": [
    "prompt = \"One day, a little girl named Lily found\"\n",
    "\n",
    "prompt = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "print(decode(generate(model, prompt, max_new_tokens=50)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
