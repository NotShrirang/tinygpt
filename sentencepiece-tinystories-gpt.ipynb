{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-09T09:36:37.643765Z",
     "iopub.status.busy": "2025-02-09T09:36:37.643324Z",
     "iopub.status.idle": "2025-02-09T09:36:44.143198Z",
     "shell.execute_reply": "2025-02-09T09:36:44.142488Z",
     "shell.execute_reply.started": "2025-02-09T09:36:37.643721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import tqdm\n",
    "import json\n",
    "import datasets\n",
    "from typing import List\n",
    "import os\n",
    "import tiktoken\n",
    "import inspect\n",
    "from sentencepiece import SentencePieceProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:36:44.144654Z",
     "iopub.status.busy": "2025-02-09T09:36:44.144260Z",
     "iopub.status.idle": "2025-02-09T09:36:44.271349Z",
     "shell.execute_reply": "2025-02-09T09:36:44.270225Z",
     "shell.execute_reply.started": "2025-02-09T09:36:44.144623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('data/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:36:44.293874Z",
     "iopub.status.busy": "2025-02-09T09:36:44.293693Z",
     "iopub.status.idle": "2025-02-09T09:36:45.732507Z",
     "shell.execute_reply": "2025-02-09T09:36:45.731543Z",
     "shell.execute_reply.started": "2025-02-09T09:36:44.293858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# encoding = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:36:45.733724Z",
     "iopub.status.busy": "2025-02-09T09:36:45.733409Z",
     "iopub.status.idle": "2025-02-09T09:36:45.739801Z",
     "shell.execute_reply": "2025-02-09T09:36:45.738999Z",
     "shell.execute_reply.started": "2025-02-09T09:36:45.733693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# encoding.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/OmAlve/TinyStories-SmolGPT/resolve/main/tok4096.model -P data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:36:45.756684Z",
     "iopub.status.busy": "2025-02-09T09:36:45.756438Z",
     "iopub.status.idle": "2025-02-09T09:36:45.768394Z",
     "shell.execute_reply": "2025-02-09T09:36:45.767510Z",
     "shell.execute_reply.started": "2025-02-09T09:36:45.756664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, tokenizer_model=\"gpt2\"):\n",
    "        self.enc = tiktoken.get_encoding(tokenizer_model)\n",
    "        self.tokenizer_model = tokenizer_model\n",
    "\n",
    "        self.n_words = self.enc.n_vocab\n",
    "        self.bos_id = None\n",
    "        self.eos_id = self.enc.eot_token\n",
    "        self.pad_id = None\n",
    "\n",
    "    def encode(self, s: str, bos: bool = False, eos: bool = False) -> List[int]:\n",
    "        t = self.enc.encode(s)\n",
    "        if bos and self.bos_id is not None:\n",
    "            t = [self.bos_id] + t\n",
    "        if eos and self.eos_id is not None:\n",
    "            t = t + [self.eos_id]\n",
    "        return t\n",
    "\n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        return self.enc.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER_MODEL = \"./data/tok4096.model\"\n",
    "TOKENIZER_MODEL = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Tokenizer:\n",
    "#     def __init__(self, tokenizer_model):\n",
    "#         model_path = tokenizer_model if tokenizer_model else TOKENIZER_MODEL\n",
    "#         self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
    "#         self.model_path = model_path\n",
    "\n",
    "#         self.n_words = self.sp_model.vocab_size()\n",
    "#         self.bos_id = self.sp_model.bos_id()\n",
    "#         self.eos_id = self.sp_model.eos_id()\n",
    "#         self.pad_id = self.sp_model.pad_id()\n",
    "\n",
    "#     def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
    "#         t = self.sp_model.encode(s)\n",
    "#         if bos:\n",
    "#             t = [self.bos_id] + t\n",
    "#         if eos:\n",
    "#             t = t + [self.eos_id]\n",
    "#         return t\n",
    "\n",
    "#     def decode(self, tokens: List[int]) -> str:\n",
    "#         return self.sp_model.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:36:45.770051Z",
     "iopub.status.busy": "2025-02-09T09:36:45.769811Z",
     "iopub.status.idle": "2025-02-09T09:36:45.781846Z",
     "shell.execute_reply": "2025-02-09T09:36:45.781243Z",
     "shell.execute_reply.started": "2025-02-09T09:36:45.770031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(tokenizer_model=TOKENIZER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:36:45.782744Z",
     "iopub.status.busy": "2025-02-09T09:36:45.782561Z",
     "iopub.status.idle": "2025-02-09T09:36:45.799592Z",
     "shell.execute_reply": "2025-02-09T09:36:45.798917Z",
     "shell.execute_reply.started": "2025-02-09T09:36:45.782727Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:36:45.800646Z",
     "iopub.status.busy": "2025-02-09T09:36:45.800384Z",
     "iopub.status.idle": "2025-02-09T09:36:45.887258Z",
     "shell.execute_reply": "2025-02-09T09:36:45.886381Z",
     "shell.execute_reply.started": "2025-02-09T09:36:45.800626Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_words\n",
    "batch_size = 12\n",
    "block_size = 512\n",
    "max_iters = 1\n",
    "eval_interval = 1000\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 256\n",
    "n_embd = 512\n",
    "n_head = 12\n",
    "n_layer = 12\n",
    "dropout = 0.3\n",
    "\n",
    "target_batch_size = 8192\n",
    "gradient_accumulation_steps = target_batch_size // batch_size\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "682"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:36:45.888277Z",
     "iopub.status.busy": "2025-02-09T09:36:45.888042Z",
     "iopub.status.idle": "2025-02-09T09:36:45.903806Z",
     "shell.execute_reply": "2025-02-09T09:36:45.903033Z",
     "shell.execute_reply.started": "2025-02-09T09:36:45.888246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:36:45.905041Z",
     "iopub.status.busy": "2025-02-09T09:36:45.904771Z",
     "iopub.status.idle": "2025-02-09T09:36:45.949409Z",
     "shell.execute_reply": "2025-02-09T09:36:45.948594Z",
     "shell.execute_reply.started": "2025-02-09T09:36:45.905007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:36:45.950677Z",
     "iopub.status.busy": "2025-02-09T09:36:45.950366Z",
     "iopub.status.idle": "2025-02-09T09:36:45.954718Z",
     "shell.execute_reply": "2025-02-09T09:36:45.953825Z",
     "shell.execute_reply.started": "2025-02-09T09:36:45.950646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def encode(s): return tokenizer.encode(s, bos=False, eos=False)\n",
    "\n",
    "def decode(l):\n",
    "\ttry:\n",
    "\t\treturn tokenizer.decode(l)\n",
    "\texcept:\n",
    "\t\treturn \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:36:45.956007Z",
     "iopub.status.busy": "2025-02-09T09:36:45.955714Z",
     "iopub.status.idle": "2025-02-09T09:37:01.388884Z",
     "shell.execute_reply": "2025-02-09T09:37:01.388238Z",
     "shell.execute_reply.started": "2025-02-09T09:36:45.955978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ds = datasets.load_dataset(\"allenai/c4\", \"realnewslike\")\n",
    "\n",
    "ds = datasets.load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:37:01.390062Z",
     "iopub.status.busy": "2025-02-09T09:37:01.389617Z",
     "iopub.status.idle": "2025-02-09T09:37:01.395742Z",
     "shell.execute_reply": "2025-02-09T09:37:01.394923Z",
     "shell.execute_reply.started": "2025-02-09T09:37:01.390026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds = ds.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:37:01.396888Z",
     "iopub.status.busy": "2025-02-09T09:37:01.396602Z",
     "iopub.status.idle": "2025-02-09T09:37:04.417459Z",
     "shell.execute_reply": "2025-02-09T09:37:04.416658Z",
     "shell.execute_reply.started": "2025-02-09T09:37:01.396859Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.\\n\\nBeep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:37:04.418564Z",
     "iopub.status.busy": "2025-02-09T09:37:04.418285Z",
     "iopub.status.idle": "2025-02-09T09:37:04.431340Z",
     "shell.execute_reply": "2025-02-09T09:37:04.430507Z",
     "shell.execute_reply.started": "2025-02-09T09:37:04.418541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    texts = [encode(item['text'])[:block_size] for item in batch]\n",
    "    padded_texts = [t + [0] * (block_size - len(t)) for t in texts]\n",
    "    return {\n",
    "        'text': torch.tensor(padded_texts, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2119719"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:37:04.432293Z",
     "iopub.status.busy": "2025-02-09T09:37:04.432102Z",
     "iopub.status.idle": "2025-02-09T09:37:04.446901Z",
     "shell.execute_reply": "2025-02-09T09:37:04.446348Z",
     "shell.execute_reply.started": "2025-02-09T09:37:04.432276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "subset_indices = list(range(eval_iters))\n",
    "# train_indices = list(range(8000000))\n",
    "# dataset_train = Subset(ds['train'], train_indices)\n",
    "dataset_valid = Subset(ds['validation'], subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:37:12.248145Z",
     "iopub.status.busy": "2025-02-09T09:37:12.247652Z",
     "iopub.status.idle": "2025-02-09T09:37:12.254332Z",
     "shell.execute_reply": "2025-02-09T09:37:12.253176Z",
     "shell.execute_reply.started": "2025-02-09T09:37:12.248101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(ds['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(dataset_valid, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:37:17.426584Z",
     "iopub.status.busy": "2025-02-09T09:37:17.426287Z",
     "iopub.status.idle": "2025-02-09T09:37:17.443342Z",
     "shell.execute_reply": "2025-02-09T09:37:17.442500Z",
     "shell.execute_reply.started": "2025-02-09T09:37:17.426561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x)  # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(\n",
    "            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,hs)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\" Root Mean Square Normalization \"\"\"\n",
    "    def __init__(self, embed_dim, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = nn.Parameter(torch.ones(embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.epsilon)\n",
    "        out = x / rms\n",
    "        out = self.gamma * out\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.ModuleList([Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.token_embedding_table.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=50, top_p=0.9, repetition_penalty=1.2):\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            if repetition_penalty != 1.0:\n",
    "                for i in range(idx.shape[0]):\n",
    "                    for token in set(idx[i].tolist()):\n",
    "                        if logits[i, token] < 0:  \n",
    "                            logits[i, token] *= repetition_penalty  \n",
    "                        else:  \n",
    "                            logits[i, token] /= repetition_penalty  \n",
    "\n",
    "            logits = logits / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            if top_k is not None:\n",
    "                top_k_values, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
    "                mask = torch.zeros_like(probs, dtype=torch.bool).scatter_(1, top_k_indices, True)\n",
    "                probs = torch.where(mask, probs, torch.tensor(0.0, device=probs.device))\n",
    "\n",
    "            if top_p is not None:\n",
    "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "                sorted_indices_to_remove[:, 0] = 0\n",
    "                for i in range(probs.size(0)):\n",
    "                    probs[i][sorted_indices[i][sorted_indices_to_remove[i]]] = 0\n",
    "\n",
    "            next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "            idx = torch.cat((idx, next_token.unsqueeze(1)), dim=-1)\n",
    "\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:37:21.232483Z",
     "iopub.status.busy": "2025-02-09T09:37:21.232153Z",
     "iopub.status.idle": "2025-02-09T09:37:21.688160Z",
     "shell.execute_reply": "2025-02-09T09:37:21.687320Z",
     "shell.execute_reply.started": "2025-02-09T09:37:21.232455Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:37:21.689737Z",
     "iopub.status.busy": "2025-02-09T09:37:21.689402Z",
     "iopub.status.idle": "2025-02-09T09:37:28.478414Z",
     "shell.execute_reply": "2025-02-09T09:37:28.477635Z",
     "shell.execute_reply.started": "2025-02-09T09:37:21.689705Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.60832 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "# model = torch.compile(model)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:37:28.480474Z",
     "iopub.status.busy": "2025-02-09T09:37:28.480046Z",
     "iopub.status.idle": "2025-02-09T09:37:28.485248Z",
     "shell.execute_reply": "2025-02-09T09:37:28.484380Z",
     "shell.execute_reply.started": "2025-02-09T09:37:28.480448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "# use_fused = fused_available and 'cuda' == str(device)\n",
    "# print(f\"{use_fused=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:37:28.487093Z",
     "iopub.status.busy": "2025-02-09T09:37:28.486844Z",
     "iopub.status.idle": "2025-02-09T09:37:28.502459Z",
     "shell.execute_reply": "2025-02-09T09:37:28.501758Z",
     "shell.execute_reply.started": "2025-02-09T09:37:28.487071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2), eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:37:28.503489Z",
     "iopub.status.busy": "2025-02-09T09:37:28.503270Z",
     "iopub.status.idle": "2025-02-09T09:37:28.519893Z",
     "shell.execute_reply": "2025-02-09T09:37:28.519178Z",
     "shell.execute_reply.started": "2025-02-09T09:37:28.503469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "T_max = len(train_dataloader)\n",
    "warmup_steps = 0.01 * T_max  # 1% of total training\n",
    "scheduler = lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=6e-4, total_steps=T_max, pct_start=0.01\n",
    ")\n",
    "\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10000, eta_min=6e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:37:28.521086Z",
     "iopub.status.busy": "2025-02-09T09:37:28.520729Z",
     "iopub.status.idle": "2025-02-09T09:37:28.535113Z",
     "shell.execute_reply": "2025-02-09T09:37:28.534482Z",
     "shell.execute_reply.started": "2025-02-09T09:37:28.521056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# eval_interval = len(train_dataloader) // 10\n",
    "# eval_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:37:28.536708Z",
     "iopub.status.busy": "2025-02-09T09:37:28.536485Z",
     "iopub.status.idle": "2025-02-09T09:37:28.549177Z",
     "shell.execute_reply": "2025-02-09T09:37:28.548366Z",
     "shell.execute_reply.started": "2025-02-09T09:37:28.536688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"ckpt/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T09:37:28.550319Z",
     "iopub.status.busy": "2025-02-09T09:37:28.550114Z",
     "iopub.status.idle": "2025-02-09T09:37:28.563541Z",
     "shell.execute_reply": "2025-02-09T09:37:28.562745Z",
     "shell.execute_reply.started": "2025-02-09T09:37:28.550300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with<|endoftext|>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = tokenizer.decode(tokenizer.encode(ds[\"train\"][0][\"text\"][:100], bos=True, eos=True))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # crop idx to the last block_size tokens\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        # get the predictions\n",
    "        logits, loss = model(idx_cond)\n",
    "        # focus only on the last time step\n",
    "        logits = logits[:, -1, :]  # becomes (B, C)\n",
    "        # apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "        # append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(682, 12, 8192)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_accumulation_steps, batch_size, target_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"losses.txt\", \"w\") as f:\n",
    "\tf.write(\"Training Loss,Validation Loss,Output\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-09T09:55:10.182Z",
     "iopub.execute_input": "2025-02-09T09:37:28.564533Z",
     "iopub.status.busy": "2025-02-09T09:37:28.564289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for iter, batch in enumerate(tqdm.notebook.tqdm(train_dataloader, total=len(train_dataloader))):\n",
    "    inputs, targets = batch['text'], batch['text']\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    with torch.autocast(device_type=str(device), dtype=torch.bfloat16):\n",
    "        logits, loss = model(inputs, targets)\n",
    "\n",
    "    loss = loss / gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    if (iter + 1) % gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "    if iter % (gradient_accumulation_steps * 2) == 0 or iter == max_iters - 1:\n",
    "        print(f\"Step {iter}: Performing validation\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            train_loss = loss.item() * gradient_accumulation_steps\n",
    "            for batch in tqdm.notebook.tqdm(valid_dataloader, total=len(valid_dataloader)):\n",
    "                inputs, targets = batch['text'], batch['text']\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                _, loss = model(inputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            torch.save(model.state_dict(), f\"ckpt/ckpt_{iter}.pt\")\n",
    "            print(f\"Train loss: {train_loss:.4f}\")\n",
    "            print(f\"Validation loss: {val_loss / len(valid_dataloader):.4f}\")\n",
    "\n",
    "            prompt = \"Hello I am \"\n",
    "            prompt = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "            output = decode(generate(model, prompt, max_new_tokens=50)[0].tolist())\n",
    "            print(output)\n",
    "            with open(\"losses.txt\", \"a\") as f:\n",
    "                f.write(f\"{train_loss},{val_loss / len(valid_dataloader)},\\\"{output}\\\"\\n\")\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T09:37:04.554910Z",
     "iopub.status.idle": "2025-02-09T09:37:04.555283Z",
     "shell.execute_reply": "2025-02-09T09:37:04.555127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"final_model_tiny_stories.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T09:37:04.556013Z",
     "iopub.status.idle": "2025-02-09T09:37:04.556379Z",
     "shell.execute_reply": "2025-02-09T09:37:04.556218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # model = GPTLanguageModel()\n",
    "# # model = model.to(device)\n",
    "# model.load_state_dict(torch.load(\"/kaggle/working/ckpt/ckpt_5625.pt\", weights_only=True))\n",
    "\n",
    "# # model.eval()\n",
    "# # model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel()\n",
    "model.load_state_dict(torch.load(\"final_model_tiny_stories.pt\", map_location=device))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T09:37:04.557508Z",
     "iopub.status.idle": "2025-02-09T09:37:04.557900Z",
     "shell.execute_reply": "2025-02-09T09:37:04.557732Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T09:37:04.558634Z",
     "iopub.status.idle": "2025-02-09T09:37:04.559007Z",
     "shell.execute_reply": "2025-02-09T09:37:04.558832Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a girl who who who who Adobe plnex sore definite \" \"\".!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"There was a girl who\"\n",
    "\n",
    "prompt = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "print(decode(generate(model, prompt, max_new_tokens=50)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T09:37:04.560013Z",
     "iopub.status.idle": "2025-02-09T09:37:04.560756Z",
     "shell.execute_reply": "2025-02-09T09:37:04.560577Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today a town in in inExperience MendNull flashlightPed Visit Thy dismasses SQLwinning shook Mitt Tem pilgrimage MOT Reggie tireRegistered kids drive even grass sounds sure but pid Sarah counties werenouemo thick SR OutlookdemocraticLTSHA conj juicyneyCredit LorenzoJÉ fundra constitutedワ\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Today a town in\"\n",
    "\n",
    "prompt = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "print(decode(generate(model, prompt, max_new_tokens=50)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-09T09:37:04.561618Z",
     "iopub.status.idle": "2025-02-09T09:37:04.562034Z",
     "shell.execute_reply": "2025-02-09T09:37:04.561843Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"final_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
